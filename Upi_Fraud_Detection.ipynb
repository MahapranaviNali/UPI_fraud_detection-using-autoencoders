# Upi_Fraud_Detection.ipynb (Python script style for export)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras import regularizers
import joblib
import pickle

# Load the dataset
df = pd.read_csv("transactions.csv")
df = df.drop(columns=["Transaction ID", "Timestamp", "Sender Name", "Receiver Name"], errors='ignore')

# Encode categorical columns
le_sender = LabelEncoder()
le_receiver = LabelEncoder()
df['Sender UPI ID'] = le_sender.fit_transform(df['Sender UPI ID'])
df['Receiver UPI ID'] = le_receiver.fit_transform(df['Receiver UPI ID'])
df['Status'] = df['Status'].apply(lambda x: 0 if x == 'SUCCESS' else 1)  # 1 = fraud

# Feature selection and scaling
features = ['Sender UPI ID', 'Receiver UPI ID', 'Amount (INR)']
X = df[features]
y = df['Status']
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
joblib.dump(scaler, "scaler.pkl")

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
X_train_normal = X_train[y_train == 0]

# Autoencoder model
input_dim = X_train.shape[1]
input_layer = Input(shape=(input_dim,))
encoded = Dense(8, activation='relu', activity_regularizer=regularizers.l1(10e-5))(input_layer)
encoded = Dense(4, activation='relu')(encoded)
decoded = Dense(8, activation='relu')(encoded)
decoded = Dense(input_dim, activation='linear')(decoded)
autoencoder = Model(inputs=input_layer, outputs=decoded)
autoencoder.compile(optimizer='adam', loss='mse')

# Train autoencoder
autoencoder.fit(X_train_normal, X_train_normal, epochs=50, batch_size=32, shuffle=True, validation_split=0.2, verbose=0)
autoencoder.save("autoencoder_model.h5")
with open("autoencoder_model.pkl", "wb") as f:
    pickle.dump(autoencoder, f)

# Prediction and error threshold
X_test_pred = autoencoder.predict(X_test)
mse = np.mean(np.power(X_test - X_test_pred, 2), axis=1)
X_train_pred = autoencoder.predict(X_train_normal)
mse_train = np.mean(np.power(X_train_normal - X_train_pred, 2), axis=1)
threshold = np.percentile(mse_train, 95)
y_pred_auto = [1 if e > threshold else 0 for e in mse]

# Autoencoder Evaluation
print("Autoencoder Classification Report:")
print(classification_report(y_test, y_pred_auto))

# Visualize reconstruction error
df_mse = pd.DataFrame({'Reconstruction Error': mse, 'True Label': y_test})
plt.figure(figsize=(10,5))
sns.histplot(mse, bins=50, kde=True)
plt.axvline(threshold, color='red', linestyle='--', label=f'Threshold: {threshold:.4f}')
plt.title("Reconstruction Error Distribution")
plt.xlabel("Reconstruction Error")
plt.ylabel("Frequency")
plt.legend()
plt.savefig("reconstruction_error.png")
plt.show()

# Comparison with other models
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
y_pred_log = log_reg.predict(X_test)

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

iso_forest = IsolationForest(contamination=0.05, random_state=42)
iso_forest.fit(X_train)
y_pred_iso = iso_forest.predict(X_test)
y_pred_iso = [1 if i == -1 else 0 for i in y_pred_iso]

# Collect results
def eval_model(y_true, y_pred):
    return {
        "Accuracy": accuracy_score(y_true, y_pred),
        "Precision": precision_score(y_true, y_pred),
        "Recall": recall_score(y_true, y_pred),
        "F1 Score": f1_score(y_true, y_pred)
    }

comparison = {
    "Autoencoder": eval_model(y_test, y_pred_auto),
    "Logistic Regression": eval_model(y_test, y_pred_log),
    "Random Forest": eval_model(y_test, y_pred_rf),
    "Isolation Forest": eval_model(y_test, y_pred_iso)
}

comparison_df = pd.DataFrame(comparison).T
print("\nModel Comparison:")
print(comparison_df)

# Plot model comparison
comparison_df.plot(kind='bar', figsize=(10,6))
plt.title("Model Evaluation Metrics")
plt.ylabel("Score")
plt.xticks(rotation=45)
plt.ylim(0, 1)
plt.tight_layout()
plt.savefig("model_comparison.png")
plt.show()
